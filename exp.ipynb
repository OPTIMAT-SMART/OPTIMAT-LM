{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16725/1323800296.py:3: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  get_ipython().magic('reset -f')\n",
      "/home/ubuntu/marbles/Projects/miniconda/envs/optimat-llm-313/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading shards:  73%|███████▎  | 22/30 [41:40<15:00, 112.53s/it]"
     ]
    }
   ],
   "source": [
    "# Clear all memory\n",
    "from IPython import get_ipython\n",
    "get_ipython().magic('reset -f')\n",
    "\n",
    "from transformers import pipeline\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import requests\n",
    "\n",
    "# Initialize the Phi-4 model pipeline\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.3-70B-Instruct\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom tool for navigation API calls\n",
    "class NavigationTool(BaseTool):\n",
    "    name: str = \"navigation\"\n",
    "    description: str = \"Validates if origin and destination addresses are valid locations\"\n",
    "    \n",
    "    def _run(self, origin: str, destination: str):\n",
    "        response = requests.post(\n",
    "            'http://127.0.0.1:5000/api/navigate',\n",
    "            json={\"origin\": origin, \"destination\": destination},\n",
    "            headers={\"Content-Type\": \"application/json\"}\n",
    "        )\n",
    "        return response.json()\n",
    "\n",
    "    def _arun(self, origin: str, destination: str):\n",
    "        raise NotImplementedError(\"Async not supported\")\n",
    "\n",
    "# Create prompt template for the agent\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    template: str\n",
    "    input_variables: list[str]\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        return self.template.format(**kwargs)\n",
    "\n",
    "template = \"\"\"You are a helpful transportation assistant. When a user asks about travel between two locations:\n",
    "1. ALWAYS extract the origin and destination\n",
    "2. Use the navigation tool with the EXACT locations mentioned\n",
    "3. Provide a helpful response based on the tool's output\n",
    "\n",
    "Example:\n",
    "Question: \"I want to go from Seattle to Portland\"\n",
    "Thought: I need to validate these locations\n",
    "Action: navigation\n",
    "Action Input: {\"origin\": \"Seattle\", \"destination\": \"Portland\"}\n",
    "\n",
    "Question: {input}\n",
    "Thought: \"\"\"\n",
    "# Initialize tools and agent\n",
    "nav_tool = NavigationTool()\n",
    "tools = [nav_tool]\n",
    "\n",
    "# Create LangChain wrapper for Phi-4\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "from langchain.agents import AgentOutputParser\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "import re\n",
    "\n",
    "class CustomOutputParser(AgentOutputParser):\n",
    "    def parse(self, llm_output: str) -> AgentAction | AgentFinish:\n",
    "        # Check if this is a final answer\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            return AgentFinish(\n",
    "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        \n",
    "        # Extract action and action input\n",
    "        action_match = re.search(r'Action: (.*?)[\\n]', llm_output, re.DOTALL)\n",
    "        action_input_match = re.search(r'Action Input: (.*)', llm_output, re.DOTALL)\n",
    "        \n",
    "        if not action_match or not action_input_match:\n",
    "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "            \n",
    "        action = action_match.group(1).strip()\n",
    "        action_input = action_input_match.group(1).strip()\n",
    "        \n",
    "        return AgentAction(tool=action, tool_input=action_input, log=llm_output)\n",
    "\n",
    "# Create the agent\n",
    "prompt = CustomPromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "output_parser = CustomOutputParser()\n",
    "\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    tools=tools,\n",
    "    stop=[\"Observation:\", \"Final Answer:\"]\n",
    ")\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# Function to handle a single chat interaction\n",
    "def handle_chat_input(user_input):\n",
    "    try:\n",
    "        response = agent_executor.invoke({\"input\": user_input})\n",
    "        return f\"Assistant: {response['output']}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Example usage in notebook:\n",
    "# response = handle_chat_input(\"I want to go from New York to Los Angeles\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "Error: '\"origin\"'\n"
     ]
    }
   ],
   "source": [
    "response = handle_chat_input(\"I want to go from New York to Los Angeles\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
